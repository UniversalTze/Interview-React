services:
# issue was not with docker caches or builds. It was with browser caches. 
# browser caches files aggressively to save bandwidth and speed up load times.
# When you first visited your app, the browser downloaded the JS bundle and saved a copy locally. 
# The next time you visited, instead of re-downloading the file, it just served the saved copy — 
# even though the file on the server had changed. The browser had no way to know the content was different
#  because the URL looked the same (/assets/index-xxx.js).
  frontend:
    x-notes: |
        The hash changing only helps if the browser actually goes back to the server to check what files are available.
        When your browser loaded the app the first time, it cached not just the JS file, but also the HTML file (index.html). 
        That HTML file is what contains the <script> tag pointing to index-OldHash.js. So when you revisited the app, 
        the browser loaded the cached HTML, saw the same old script tag pointing to index-OldHash.js, and served 
        that from cache too — it never even looked at the new HTML that pointed to index-Jpy1gUh8.js.

        Chain of Failure:
        1. Browser loads cached index.html → sees reference to index-OldHash.js 
        2. Browser loads cached index-OldHash.js → old URL, old bundle, done
        3. New index.html and index-NewHash.js sitting on the server, never consulted
        The proper fix for production apps is to configure your server (in your case nginx) to tell the browser never to cache index.html, 
        but to cache JS/CSS files aggressively since those have hashed names. That way the HTML is always fresh, the new hash is 
        always discovered, and cache busting works as intended automatically.

    image: ready-set-hire-frontend
    build:
      context: . 
      # above the is the curent context given wehn file is ran
      dockerfile: Dockerfile.frontend
    ports:
      - "8080:80"
  db:
    image: postgres:16 # postgress image on docker
    restart: always    # restart if error occurs
    environment: # env variables
      POSTGRES_PASSWORD: ReadyTesting.1823
      #POSTGRES_USER: superReadyUser
      POSTGRES_DB: ReadySetHire

    healthcheck: 
      # runs subsequent commands in a bash shell below
      test: ["CMD-SHELL", "pg_isready -U postgrest -d ReadySetHire"]
      interval: 5s
      retries: 10
    volumes: # bind mount (db so that it exists in container)
      - ./backend/db:/docker-entrypoint-initdb.d 
    # this is for it to exist after (named volume)
    #  - db-data:/var/lib/postgresql/data

  postgrest:
    image: postgrest/postgrest:latest
    depends_on:
      db:
        condition: service_healthy
    environment: # opened postgrest container port:5432 for postgress trafic
      PGRST_DB_URI: "postgres://postgrest:postgrestpw@db:5432/ReadySetHire"
      PGRST_DB_SCHEMA: "readysethire"
      PGRST_DB_ANON_ROLE: "webdemo"
    ports:
      - "3000:3000"

#volumes: (Stored in linux space allocated by docker)
#  db-data:"
    